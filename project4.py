# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OfNFSIrEDZY3kl_XiP_fMqrw6grtWevL
"""

!pip install torch torchvision numpy matplotlib
!pip install tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.transforms import ToTensor
from PIL import Image
import matplotlib.pyplot as plt

# Define DIP class and training function (unchanged)
class DIP(nn.Module):
    def __init__(self):
        super(DIP, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 3, kernel_size=3, padding=1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.conv3(x)
        return x

def train_DIP(target_image, num_epochs=1000, learning_rate=0.01):
    model = DIP()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()

    target_image = ToTensor()(target_image).unsqueeze(0)
    input_noise = torch.randn_like(target_image)

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        output = model(input_noise)
        loss = criterion(output, target_image)
        loss.backward()
        optimizer.step()

        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item()}')

    return model(input_noise).detach()

# Load the Lena image and train DIP
target_image = Image.open('Lenna.png')
output_image = train_DIP(target_image)

# Display the result
plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.title('Original Image')
plt.imshow(target_image)
plt.subplot(1, 2, 2)
plt.title('DIP Output')
plt.imshow(output_image.squeeze().permute(1, 2, 0).numpy())
plt.show()

# Define DDPM class with a noise prediction network
class DDPM(nn.Module):
    def __init__(self, beta_start=1e-4, beta_end=0.02, num_timesteps=1000):
        super(DDPM, self).__init__()
        self.num_timesteps = num_timesteps
        self.beta = torch.linspace(beta_start, beta_end, num_timesteps)
        self.alpha = 1 - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)

        # Simple noise prediction network
        self.noise_predictor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 3, kernel_size=3, padding=1)
        )

    def forward_diffusion(self, x_0, t):
        noise = torch.randn_like(x_0)
        return torch.sqrt(self.alpha_hat[t]) * x_0 + torch.sqrt(1 - self.alpha_hat[t]) * noise

    def reverse_diffusion(self, x_t, t):
        pred_noise = self.noise_predictor(x_t)
        return (x_t - torch.sqrt(1 - self.alpha_hat[t]) * pred_noise) / torch.sqrt(self.alpha_hat[t])

def train_DDPM(initial_prior, num_epochs=500, learning_rate=0.01):
    model = DDPM()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()

    initial_prior = initial_prior.unsqueeze(0)  # Ensure the initial prior is in the correct shape

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        t = torch.randint(0, model.num_timesteps, (1,))
        x_t = model.forward_diffusion(initial_prior, t)
        x_0_pred = model.reverse_diffusion(x_t, t)
        loss = criterion(x_0_pred, initial_prior)
        loss.backward()
        optimizer.step()

        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item()}')

    return model

# Use the DIP output as an initial prior for DDPM
initial_prior = output_image.squeeze(0)  # Remove batch dimension for consistency
ddpm_model = train_DDPM(initial_prior)

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.transforms import ToTensor
from PIL import Image
import matplotlib.pyplot as plt

# Define DIP class and training function
class DIP(nn.Module):
    def __init__(self):
        super(DIP, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 3, kernel_size=3, padding=1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.conv3(x)
        return x

def train_DIP(target_image, num_epochs=500, learning_rate=0.01):
    model = DIP()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()

    target_image = ToTensor()(target_image).unsqueeze(0)
    input_noise = torch.randn_like(target_image)

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        output = model(input_noise)
        loss = criterion(output, target_image)
        loss.backward()
        optimizer.step()

        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item()}')

    return model(input_noise).detach()

# Load the Lena image and train DIP
target_image = Image.open('Lenna.png')
output_image = train_DIP(target_image)

# Display the result
plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.title('Original Image')
plt.imshow(target_image)
plt.subplot(1, 2, 2)
plt.title('DIP Output')
plt.imshow(output_image.squeeze().permute(1, 2, 0).numpy())
plt.show()

# Define DDPM class with a noise prediction network
class DDPM(nn.Module):
    def __init__(self, beta_start=1e-4, beta_end=0.02, num_timesteps=1000):
        super(DDPM, self).__init__()
        self.num_timesteps = num_timesteps
        self.beta = torch.linspace(beta_start, beta_end, num_timesteps)
        self.alpha = 1 - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)

        # Simple noise prediction network
        self.noise_predictor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 3, kernel_size=3, padding=1)
        )

    def forward_diffusion(self, x_0, t):
        noise = torch.randn_like(x_0)
        return torch.sqrt(self.alpha_hat[t]) * x_0 + torch.sqrt(1 - self.alpha_hat[t]) * noise

    def reverse_diffusion(self, x_t, t):
        pred_noise = self.noise_predictor(x_t)
        return (x_t - torch.sqrt(1 - self.alpha_hat[t]) * pred_noise) / torch.sqrt(self.alpha_hat[t])

def train_DDPM(initial_prior, num_epochs=500, learning_rate=0.01):
    model = DDPM()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()

    initial_prior = initial_prior.unsqueeze(0)  # Ensure the initial prior is in the correct shape

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        t = torch.randint(0, model.num_timesteps, (1,))
        x_t = model.forward_diffusion(initial_prior, t)
        x_0_pred = model.reverse_diffusion(x_t, t)
        loss = criterion(x_0_pred, initial_prior)
        loss.backward()
        optimizer.step()

        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item()}')

    return model

# Use the DIP output as an initial prior for DDPM
initial_prior = output_image.squeeze(0)  # Remove batch dimension for consistency
ddpm_model = train_DDPM(initial_prior)